{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "editable": false, "jupyter": {"outputs_hidden": true, "source_hidden": true}}, "outputs": [], "source": ["# Please do not change this cell because some hidden tests might depend on it.\n", "import os\n", "\n", "# Otter grader does not handle ! commands well, so we define and use our\n", "# own function to execute shell commands.\n", "def shell(commands, warn=True):\n", "    \"\"\"Executes the string `commands` as a sequence of shell commands.\n", "     \n", "       Prints the result to stdout and returns the exit status. \n", "       Provides a printed warning on non-zero exit status unless `warn` \n", "       flag is unset.\n", "    \"\"\"\n", "    file = os.popen(commands)\n", "    print (file.read().rstrip('\\n'))\n", "    exit_status = file.close()\n", "    if warn and exit_status != None:\n", "        print(f\"Completed with errors. Exit status: {exit_status}\\n\")\n", "    return exit_status\n", "\n", "shell(\"\"\"\n", "ls requirements.txt >/dev/null 2>&1\n", "if [ ! $? = 0 ]; then\n", " rm -rf .tmp\n", " git clone https://github.com/cs187-2020/lab2-5.git .tmp\n", " mv .tmp/tests ./\n", " mv .tmp/requirements.txt ./\n", " rm -rf .tmp\n", "fi\n", "pip install -q -r requirements.txt\n", "\"\"\")"]}, {"cell_type": "code", "execution_count": null, "id": "ed037b05", "metadata": {"deletable": false, "editable": false}, "outputs": [], "source": ["# Initialize Otter\n", "import otter\n", "grader = otter.Notebook()"]}, {"cell_type": "raw", "metadata": {"id": "dLz7tmLAsIYn", "jupyter": {"source_hidden": true}}, "source": ["%%latex\n", "\\newcommand{\\vect}[1]{\\mathbf{#1}}\n", "\\newcommand{\\cnt}[1]{\\sharp(#1)}\n", "\\newcommand{\\argmax}[1]{\\underset{#1}{\\operatorname{argmax}}}\n", "\\newcommand{\\softmax}{\\operatorname{softmax}}\n", "\\newcommand{\\Prob}{\\Pr}\n", "\\newcommand{\\given}{\\,|\\,}"]}, {"cell_type": "markdown", "metadata": {"id": "xQVfC9hHsC5j", "jupyter": {"source_hidden": true}}, "source": ["$$\n", "\\renewcommand{\\vect}[1]{\\mathbf{#1}}\n", "\\renewcommand{\\cnt}[1]{\\sharp(#1)}\n", "\\renewcommand{\\argmax}[1]{\\underset{#1}{\\operatorname{argmax}}}\n", "\\renewcommand{\\softmax}{\\operatorname{softmax}}\n", "\\renewcommand{\\Prob}{\\Pr}\n", "\\renewcommand{\\given}{\\,|\\,}\n", "$$"]}, {"cell_type": "markdown", "metadata": {"id": "qSwCXUcAsJOW"}, "source": ["# CS187\n", "## Lab 2-5 - Sequence labeling with recurrent neural networks\n", "\n", "In the last lab, you saw how to use hidden Markov models (HMMs) for sequence labeling. In this lab, you will use recurrent neural networks (RNNs) for sequence labeling. "]}, {"cell_type": "markdown", "metadata": {"id": "-GF1mE2tUz-p"}, "source": ["In this lab, we consider the task of automatic punctuation restoration from unpunctuated text, which is useful for post-processing transcribed speech from speech recognition systems (since we don't want users to have to utter all punctuation marks). We can formulate this task as a sequence labeling task, predicting for each word the punctuation that should follow. If there's no punctuation followign the word, we use a special tag `O` for \"other\".\n", "\n", "The dataset we use is the Federalist papers, but this time we use text without punctuation as our input, and predict the punctuation following each word. An example from the dataset looks like below, which correponds to the punctuated sentence `<bos> to the people of the state of new york : ` (notice the ending colon):\n", "\n", "| Token   | Label    |\n", "| ------- | ------ |\n", "| \\<bos\\> | O |\n", "| to | O |\n", "| the | O |\n", "| people | O |\n", "| of | O |\n", "| the | O |\n", "| state | O |\n", "| of | O |\n", "| new | O |\n", "| york | : |"]}, {"cell_type": "markdown", "metadata": {"id": "m0pV0ytYYW9z"}, "source": ["### Setup"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 35}, "deletable": false, "editable": false, "id": "Gzm8d7_k0_pe", "outputId": "bad64928-d199-4993-d377-63da32be9b78"}, "outputs": [], "source": ["import copy\n", "\n", "import torch\n", "import torchtext.legacy as tt\n", "import torch.nn as nn\n", "import warnings\n", "\n", "from collections import Counter\n", "from tqdm import tqdm"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 35}, "id": "Gzm8d7_k0_pe", "outputId": "bad64928-d199-4993-d377-63da32be9b78"}, "outputs": [], "source": ["## GPU check\n", "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n", "print(device)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 70}, "id": "mGlCYzK20kDp", "outputId": "ebe50d16-a737-4945-ea79-6fd0e45fea8b"}, "outputs": [], "source": ["# Download data\n", "shell(\"\"\"\n", "  wget -nv -N -P data https://raw.githubusercontent.com/nlp-course/data/master/Federalist/federalist_tag.train.txt\n", "  wget -nv -N -P data https://raw.githubusercontent.com/nlp-course/data/master/Federalist/federalist_tag.dev.txt\n", "  wget -nv -N -P data https://raw.githubusercontent.com/nlp-course/data/master/Federalist/federalist_tag.test.txt\n", "\"\"\")"]}, {"cell_type": "markdown", "metadata": {"id": "7ExnOLb4Yz3u"}, "source": ["As before, we use `torchtext` to load data. We use one field for processing the words (`TEXT`), and another for processing the tags (`TAG`)."]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 196}, "id": "Yceh3yopww2k", "outputId": "def67b6e-7ead-41c4-cd92-8721480dab7a"}, "outputs": [], "source": ["# We place a limit on the size of the vocabulary, including only the \n", "# `MAX_VOCAB_SIZE` most frequent words. All others will become <unk>.\n", "MAX_VOCAB_SIZE = 5000\n", "\n", "# Create fields\n", "TEXT = tt.data.Field(sequential=True, \n", "                     include_lengths=False, \n", "                     batch_first=False,\n", "                     tokenize=None)\n", "TAG = tt.data.Field(sequential=True, \n", "                    include_lengths=False, \n", "                    batch_first=False,\n", "                    tokenize=None)\n", "fields = (('text', TEXT), ('tag', TAG))\n", "\n", "# Load data\n", "tt.datasets.SequenceTaggingDataset.name = 'seq'\n", "train_data, val_data, test_data = tt.datasets.SequenceTaggingDataset.splits(\n", "            fields=fields, \n", "            path='data/', \n", "            train='federalist_tag.train.txt', \n", "            validation='federalist_tag.dev.txt',\n", "            test='federalist_tag.test.txt')\n", "\n", "# Build vocabulary\n", "TEXT.build_vocab(train_data.text, max_size=MAX_VOCAB_SIZE)\n", "TAG.build_vocab(train_data.tag)\n", "\n", "# Print out some stats\n", "vocab_size = len(TEXT.vocab.itos)\n", "num_labels = len(TAG.vocab.itos)\n", "\n", "print (f\"Size of English vocabulary: {len(TEXT.vocab)}\")\n", "print (f\"Most common English words: {TEXT.vocab.freqs.most_common(10)}\\n\")\n", "\n", "print (f\"Number of tags: {len(TAG.vocab)}\")\n", "print (f\"Most common tags: {TAG.vocab.freqs.most_common(10)}\")"]}, {"cell_type": "markdown", "metadata": {"id": "VwPQCP2QrVZT"}, "source": ["You can see from above that the most common punctuation is comma, on which we will evaluate precision, recall, and F-1 scores later."]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 35}, "id": "BVz7og2wqNer", "outputId": "f47bdbd5-1085-4354-a290-c3b4d270b1b6"}, "outputs": [], "source": ["comma_id = TAG.vocab.stoi[',']\n", "print (f\"Id of comma: {comma_id}\")"]}, {"cell_type": "markdown", "metadata": {"id": "L_DSH_rSJ_IL"}, "source": ["We mapped words that are not among the most frequent words (specified by `MAX_VOCAB_SIZE`) to a special unknown token:"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 52}, "id": "A5Nrc7r6KUtA", "outputId": "c6212abf-ff77-4ede-960a-2d99f989fc9b"}, "outputs": [], "source": ["unk_type = TEXT.unk_token\n", "unk_index = TEXT.vocab.stoi[unk_type]\n", "\n", "print (f\"Unknown word: {unk_type}\\n\"\n", "       f\"Unknown index: {unk_index}\")"]}, {"cell_type": "markdown", "metadata": {"id": "V6TZ-GMYtmlU"}, "source": ["To load data in batched tensors, we use `data.BucketIterator` for the training and validation set, which enables us to iterate over the dataset under a given `BATCH_SIZE`, which is set to be `1` throughout this lab. We still batch the data because other torch functions expect data to be batched.\n", "\n", "For the test set, we use `data.Iterator`, which doesn't batch the data, since we'll be performing the evaluation on the test set ourselves."]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 108}, "id": "blqQPwj-ZFiN", "outputId": "e5e82d39-5917-4263-ea81-2fdb41fb07b1"}, "outputs": [], "source": ["BATCH_SIZE = 1 # we use batch size 1 for simplicity\n", "\n", "train_iter = tt.data.BucketIterator(train_data, batch_size=BATCH_SIZE, device=device)\n", "val_iter = tt.data.BucketIterator(val_data, batch_size=BATCH_SIZE, device=device)\n", "test_iter = tt.data.Iterator(test_data, batch_size=BATCH_SIZE, sort=False, device=device)"]}, {"cell_type": "markdown", "metadata": {"id": "WGFUInaFamth"}, "source": ["Let's take a look at the dataset. Recall from project 1 that there are two different ways of iterating over the dataset, one by iterating over individual examples, the other by iterating over batches of examples."]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 688}, "id": "5WIrJzZ2bnPG", "outputId": "4f8d361e-1327-4b35-e0be-e362380601ff"}, "outputs": [], "source": ["# Iterating over individual examples:\n", "# Note that the words are the original words, so you'd need to manually \n", "# replace them with <unk> if not in the vocabulary.\n", "example = train_iter.dataset[1]\n", "text = example.text # a sequence of unpunctuated words\n", "tags = example.tag  # a sequence of tags indicating the proper punctuation\n", "print (f'{\"TYPE\":15}: {\"TAG\"}')\n", "for word, tag in zip(text, tags):\n", "  print (f'{word:15}: {tag}')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Alternatively, we can produce the data a batch at a time, as in the example below. Note the \"shape\" of a batch, it's a two-dimensional tensor of size `max_length x batch_size`. (In this case, `batch_size` is 1.) Thus, to extract a sentence from a batch, we need to index by the _second_ dimension, not the first, using the Python idiom `batch[:, 0]`."]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 619}, "id": "xu5qSLFVcQPD", "outputId": "fc2236c1-1109-4bae-9918-79fa6de7b713"}, "outputs": [], "source": ["# Iterating over batches of examples:\n", "# Unknown words have been mapped to unknown word ids\n", "batch = next(iter(train_iter))\n", "text = batch.text\n", "example_text = text[:, 0]\n", "print (f\"Size of text batch: {text.size()}\")\n", "print (f\"First sentence in batch: {example_text}\")\n", "print (f\"Converted back to string: {' '.join([TEXT.vocab.itos[word_id] for word_id in example_text])}\")\n", "\n", "print ('-'*20)\n", "tags = batch.tag\n", "example_tags = tags[:, 0]\n", "print (f\"Size of label batch: {tags.size()}\")\n", "print (f\"Tags of the first sentence in batch: {example_tags}\")\n", "print (f\"Converted back to string: {' '.join([TAG.vocab.itos[tag_id] for tag_id in example_tags])}\")"]}, {"cell_type": "markdown", "metadata": {"id": "KsTcHp_YZDSE"}, "source": ["Given the tags of an unpunctuated sequence of words, we can easily restore the punctuation:"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "_BIiUVNfZ3vn"}, "outputs": [], "source": ["def restore_punctuation(words, tags):\n", "  words_with_punc = []\n", "  for word, tag in zip(words, tags):\n", "    words_with_punc.append(word)\n", "    if tag != 'O':\n", "      words_with_punc.append(tag)\n", "  return ' '.join(words_with_punc)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 55}, "id": "5JnpFCD_bFWX", "outputId": "fc3ff09c-b362-43d3-d3c8-af76b18763a8"}, "outputs": [], "source": ["print(restore_punctuation(example.text, example.tag))"]}, {"cell_type": "markdown", "metadata": {"id": "4QYZw7E-lOdy"}, "source": ["### Majority Labeling\n", "\n", "Recall from our previous lab that a naive baseline is choosing the majority label for each word in the sequence, where the majority label depends on the word. We've provided an implementation of this baseline for you, to give you a sense of how difficult the punctuation restoration task is."]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "PPSJYcr2JAsw"}, "outputs": [], "source": ["class MajorityTagger():\n", "  def __init__(self):\n", "    \"\"\"Initializer.\n", "    \"\"\"\n", "    self.most_common_label_given_word = {}\n", "\n", "  def train_all(self, train_iter):\n", "    \"\"\"Finds the majority label for each word in the training set.\n", "    \"\"\"\n", "    train_counts_given_word = {}\n", "    for ex in train_iter.dataset:\n", "      for word, tag in zip(ex.text, ex.tag):\n", "        if TEXT.vocab.stoi[word] == unk_index:\n", "          word = unk_type\n", "        if word not in train_counts_given_word:\n", "          train_counts_given_word[word] = Counter([])\n", "        train_counts_given_word[word].update([tag])\n", "    \n", "    for word in train_counts_given_word:\n", "      # Find the most common\n", "      most_common_label = train_counts_given_word[word].most_common(1)[0][0]\n", "      self.most_common_label_given_word[word] = most_common_label\n", "\n", "  def predict_all(self, test_iter):\n", "    \"\"\"Predicts labels for each example in test_iter.\n", "       Returns a list of list of strings. The order should be the same as\n", "       in `test_iter.dataset` (or equivalently `test_iter`).\n", "    \"\"\"\n", "    predictions = []\n", "    for example in test_iter.dataset:\n", "      words = example.text\n", "      tags_pred = []\n", "      for word in words:\n", "        if TEXT.vocab.stoi[word] == unk_index:\n", "          word = unk_type\n", "        tag_pred = self.most_common_label_given_word[word]\n", "        tags_pred.append(tag_pred)\n", "      predictions.append(tags_pred)\n", "    return predictions # a list of lists of strings\n", "\n", "  def evaluate(self, test_iter):\n", "    \"\"\"Evaluates the overall accuracy, and the precision and recall of comma.\n", "    \"\"\"\n", "    correct = 0\n", "    total = 0\n", "    true_positive_comma = 0\n", "    predicted_positive_comma = 0\n", "    total_positive_comma = 0\n", "\n", "    # Get predictions\n", "    predictions = self.predict_all(test_iter)\n", "    assert len(predictions) == len(test_iter.dataset)\n", "    \n", "    for tags_pred, example in zip(predictions, test_iter.dataset):\n", "      tags = example.tag\n", "      assert len(tags_pred) == len(tags)\n", "      for tag_pred, tag in zip(tags_pred, tags):\n", "        total += 1\n", "        if tag_pred == tag:\n", "          correct += 1\n", "        if tag_pred == ',':\n", "          predicted_positive_comma += 1 # predicted positive\n", "        if tag == ',':\n", "          total_positive_comma += 1     # gold label positive\n", "        if tag_pred == ',' and tag == ',':\n", "          true_positive_comma += 1      # true positive\n", "    precision_comma = true_positive_comma / predicted_positive_comma\n", "    recall_comma = true_positive_comma / total_positive_comma\n", "    F1_comma = 2. / (1./precision_comma + 1./recall_comma)\n", "    return correct/total, precision_comma, recall_comma, F1_comma"]}, {"cell_type": "markdown", "metadata": {"id": "Ze0oaDFWcYCd"}, "source": ["Now, we can train our baseline on training data. "]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "FrwTRPQCccJV"}, "outputs": [], "source": ["maj_tagger = MajorityTagger()\n", "maj_tagger.train_all(train_iter)"]}, {"cell_type": "markdown", "metadata": {"id": "2iR7HlrPcKyo"}, "source": ["Let's take a look at an example prediction using this simple baseline."]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 108}, "id": "wuEV1_XbcSbo", "outputId": "dbdb06a5-3a10-4801-df03-3d69f366feb5"}, "outputs": [], "source": ["# Get all predictions\n", "predictions = maj_tagger.predict_all(test_iter)\n", "\n", "# Pick one example\n", "example_id = 2 # the third example\n", "example = test_iter.dataset[example_id]\n", "prediction = predictions[example_id]\n", "\n", "print('Ground truth punctuation:')\n", "print(restore_punctuation(example.text, example.tag), '\\n')\n", "print('Predicted punctuation:')\n", "print(restore_punctuation(example.text, prediction))"]}, {"cell_type": "markdown", "metadata": {"id": "rb2GSwX_X_WT"}, "source": ["This baseline model clearly grossly underpunctuates. It predicts the tag to be `O` almost all of the time.\n", "\n", "We can quantitatively evaluate the performance of the majority labeling tagger, which establishes a baseline that any reasonable model should outperform."]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 52}, "id": "hUHLm---Lp4o", "outputId": "8b402e32-c3ff-4118-fcaf-82af25095429"}, "outputs": [], "source": ["accuracy, precision_comma, recall_comma, F1_comma = maj_tagger.evaluate(test_iter)\n", "print (f\"Overall Accuracy: {accuracy:.4f}. \\n\"\n", "       f\"Comma: Precision: {precision_comma:.4f}. Recall: {recall_comma:.4f}. F1: {F1_comma:.4f}\")"]}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false, "id": "64HhdtObXprN"}, "source": ["<!-- BEGIN QUESTION -->\n", "\n", "**Question:** You can see that even though the overall accuracy is pretty high, the F-1 score of comma is very low. Why?\n", "\n", "<!--\n", "BEGIN QUESTION\n", "name: open_response_F1\n", "manual: true\n", "-->"]}, {"cell_type": "markdown", "id": "dadb3be5", "metadata": {}, "source": ["_Type your answer here, replacing this text._"]}, {"cell_type": "markdown", "metadata": {"id": "ZEtg2j0hYmED"}, "source": ["<!-- END QUESTION -->\n", "\n", "\n", "\n", "### RNN Sequence Tagging\n", "\n", "Now we get to the real point, using an RNN model for sequence tagging. We provide a base class `RNNBaseTagger` below, which implements training and evaluation. Throughout the rest of this lab, you will implement three subclasses of this class, using PyTorch functions at different abstraction levels."]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "iTcRPwuN8dWW"}, "outputs": [], "source": ["class RNNBaseTagger(nn.Module):\n", "  def __init__(self):\n", "    super().__init__()\n", "\n", "  def init_parameters(self, init_low=-0.15, init_high=0.15):\n", "    \"\"\"Initialize parameters. We usually use larger initial values for smaller models.\n", "    See http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf for a more\n", "    in-depth discussion.\n", "    \"\"\"\n", "    for p in self.parameters():\n", "      p.data.uniform_(init_low, init_high)\n", "\n", "  def forward(self, text_batch):\n", "    \"\"\"Performs forward, returns logits.\n", "    \n", "    Arguments: \n", "      text_batch: a tensor of size (seq_len, 1) \n", "    Returns:\n", "      logits: a tensor of size (seq_len, 1, self.N)\n", "    \"\"\"\n", "    raise NotImplementedError\n", "\n", "  def train_all(self, train_iter, val_iter, epochs=5, learning_rate=1e-3):\n", "    # Switch the module to training mode\n", "    self.train()\n", "    # Use Adam to optimize the parameters\n", "    optim = torch.optim.Adam(self.parameters(), lr=learning_rate)\n", "    best_validation_accuracy = -float('inf')\n", "    best_model = None\n", "    # Run the optimization for multiple epochs\n", "    for epoch in range(epochs): \n", "      total = 0\n", "      running_loss = 0.0\n", "      for batch in tqdm(train_iter):\n", "        # Zero the parameter gradients\n", "        self.zero_grad()\n", "\n", "        # Input and target\n", "        words = batch.text # seq_len, 1\n", "        tags = batch.tag   # seq_len, 1\n", "        \n", "        # Run forward pass and compute loss along the way.\n", "        logits = self.forward(words)\n", "        loss = self.loss_function(logits.view(-1, self.N), tags.view(-1))\n", "        \n", "        # Perform backpropagation\n", "        (loss/words.size(1)).backward()\n", "\n", "        # Update parameters\n", "        optim.step()\n", "\n", "        # Training stats\n", "        total += 1\n", "        running_loss += loss.item()\n", "        \n", "      # Evaluate and track improvements on the validation dataset\n", "      validation_accuracy, _, _, _ = self.evaluate(val_iter)\n", "      if validation_accuracy > best_validation_accuracy:\n", "        best_validation_accuracy = validation_accuracy\n", "        self.best_model = copy.deepcopy(self.state_dict())\n", "      epoch_loss = running_loss / total\n", "      print (f'Epoch: {epoch} Loss: {epoch_loss:.4f} '\n", "             f'Validation accuracy: {validation_accuracy:.4f}')\n", "\n", "  def predict(self, text_batch):\n", "    \"\"\"Returns the most likely sequence of tags for a sequence of words in `text_batch`.\n", "\n", "    Arguments: \n", "      text_batch: a tensor containing word ids of size (seq_len, 1) \n", "    Returns:\n", "      tag_batch: a tensor containing tag ids of size (seq_len, 1)\n", "    \"\"\"\n", "    raise NotImplementedError\n", "\n", "  def evaluate(self, iterator):\n", "    \"\"\"Returns the model's performance on a given dataset `iterator`.\n", "\n", "    Arguments: \n", "      iterator\n", "    Returns:\n", "      overall accuracy, and precision, recall, and F1 for comma\n", "    \"\"\"\n", "    correct = 0\n", "    total = 0\n", "    true_positive_comma = 0\n", "    predicted_positive_comma = 0\n", "    total_positive_comma = 0\n", "    pad_id = TAG.vocab.stoi[TAG.pad_token]\n", "    for batch in tqdm(iterator):\n", "      words = batch.text\n", "      tags = batch.tag\n", "      tags_pred = self.predict(words)\n", "      mask = tags.ne(pad_id)\n", "      cor = (tags == tags_pred)[mask]\n", "      correct += cor.float().sum().item()\n", "      total += mask.float().sum().item()\n", "      predicted_positive_comma += (mask * tags_pred.eq(comma_id)).float().sum().item()\n", "      true_positive_comma += (mask * tags.eq(comma_id) * tags_pred.eq(comma_id)).float().sum().item()\n", "      total_positive_comma += (mask * tags.eq(comma_id)).float().sum().item()\n", "\n", "    precision_comma = true_positive_comma / predicted_positive_comma\n", "    recall_comma = true_positive_comma / total_positive_comma\n", "    F1_comma = 2. / (1./precision_comma + 1./recall_comma)\n", "    return correct/total, precision_comma, recall_comma, F1_comma"]}, {"cell_type": "markdown", "metadata": {"id": "KW7YJDvd9RI7"}, "source": ["#### RNN from scratch\n", "\n", "In this part of the lab, you will implement the forward pass of an RNN from scratch. Recall that \n", "\n", "\\begin{align}\n", "h_0 &= 0\\\\\n", "h_t &= \\sigma(\\vect{U}x_t + \\vect{V}h_{t - 1} + b_h) \\\\\n", "o_t &= \\vect{W}h_t + b_o\n", "\\end{align}\n", "\n", "where we embed each word and use its embedding as $x_t$, and we use $o_t$ as the output logits. (Again, the final softmax has been absorbed into the loss function so you don't need to implement that.) Note that we added bias vectors $b_h$ and $b_o$ in this lab since we are training very small models. (In large models, having a bias vector matters a lot less.) \n", "\n", "You will need to implement both the `forward` function and the `predict` function.\n", "\n", "> Hint: You might find [`torch.stack`](https://pytorch.org/docs/stable/generated/torch.stack.html) useful for stacking a list of tensors to form a single tensor. You can also use `torch.mv` or `@` for matrix-vector multiplication, `torch.mm` for matrix-matrix multiplication.\n", "\n", "**Warning: Training this and later models takes a little while, likely around three minutes for the full set of epochs. You might want to set the number of epochs to a small number (1?) until your code is running well. You should also feel free to move ahead to the next parts while earlier parts are running.**"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "ITvJXi8hiCOy"}, "outputs": [], "source": ["class RNNTagger1(RNNBaseTagger):\n", "  def __init__(self, text, tag, embedding_size, hidden_size):\n", "    super().__init__()\n", "    self.text = text\n", "    self.tag = tag\n", "    self.N = len(tag.vocab.itos)   # tag vocab size\n", "    self.V = len(text.vocab.itos)  # text vocab size\n", "    self.embedding_size = embedding_size\n", "    self.hidden_size = hidden_size\n", "\n", "    # Create essential modules\n", "    self.word_embeddings = nn.Embedding(self.V, embedding_size) # Lookup layer\n", "    self.U = nn.Parameter(torch.Tensor(hidden_size, embedding_size))\n", "    self.V = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n", "    self.b_h = nn.Parameter(torch.Tensor(hidden_size))\n", "    self.sigma = nn.Tanh() # Nonlinear Layer\n", "    self.W = nn.Parameter(torch.Tensor(self.N, hidden_size))\n", "    self.b_o = nn.Parameter(torch.Tensor(self.N))\n", "\n", "    # Create loss function\n", "    pad_id = self.tag.vocab.stoi[self.tag.pad_token]\n", "    self.loss_function = nn.CrossEntropyLoss(reduction='sum', ignore_index=pad_id)\n", "\n", "    # Initialize parameters\n", "    self.init_parameters()\n", "\n", "  def forward(self, text_batch):\n", "    \"\"\"Performs forward, returns logits.\n", "    \n", "    Arguments: \n", "      text_batch: a tensor of size (seq_len, 1) \n", "    Returns:\n", "      logits: a tensor of size (seq_len, 1, self.N)\n", "    \"\"\"\n", "    h0 = torch.zeros(self.hidden_size, device=device)\n", "    word_embeddings = self.word_embeddings(text_batch) # seq_len, 1, embedding_size\n", "    seq_len = word_embeddings.size(0)\n", "    #TODO: your code below\n", "    logits = ...\n", "    return logits\n", "\n", "  def predict(self, text_batch):\n", "    \"\"\"Returns the most likely sequence of tags for a sequence of words in `text_batch`.\n", "\n", "    Arguments: \n", "      text_batch: a tensor containing word ids of size (seq_len, 1) \n", "    Returns:\n", "      tag_batch: a tensor containing tag ids of size (seq_len, 1)\n", "    \"\"\"\n", "    #TODO: your code below\n", "    tag_batch = ...\n", "    return tag_batch"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 425}, "id": "oeJBvq-AiIJJ", "outputId": "6ecf2d7a-715e-47a8-aac0-161f2088d20c"}, "outputs": [], "source": ["# Instantiate and train classifier\n", "rnn_tagger1 = RNNTagger1(TEXT, TAG, embedding_size=32, hidden_size=32).to(device)\n", "rnn_tagger1.train_all(train_iter, val_iter, epochs=5, learning_rate=1e-3)\n", "rnn_tagger1.load_state_dict(rnn_tagger1.best_model)\n", "\n", "# Evaluate model performance\n", "train_accuracy1, train_p1, train_r1, train_f1 = rnn_tagger1.evaluate(train_iter)\n", "test_accuracy1, test_p1, test_r1, test_f1 = rnn_tagger1.evaluate(test_iter)\n", "print(f'\\nTraining accuracy: {train_accuracy1:.3f}, precision: {train_p1:.3f}, recall: {train_r1:.3f}, F-1: {train_f1:.3f}\\n'\n", "      f'Test accuracy: {test_accuracy1:.3f}, precision: {test_p1:.3f}, recall: {test_r1:.3f}, F-1: {test_f1:.3f}')"]}, {"cell_type": "code", "execution_count": null, "id": "44e8f2ca", "metadata": {"deletable": false, "editable": false}, "outputs": [], "source": ["grader.check(\"rnn1\")"]}, {"cell_type": "markdown", "metadata": {"id": "HiJkTGxIrL9D"}, "source": ["Did your model outperform the baseline? Don't be surprised if it doesn't: the model is very small and the dataset is small as well."]}, {"cell_type": "markdown", "metadata": {"id": "FVzvJmge9d1t"}, "source": ["#### RNN forward using `nn.RNN` and explicit loop through time steps\n", "\n", "In this part, you will use `nn.RNN` and `nn.Linear` to implement the forward pass:\n", "\n", "\\begin{align}\n", "h_0 &= 0\\\\\n", "h_t &= \\text{nn.RNN}(x_t,h_{t - 1}) \\\\\n", "o_t &= \\text{nn.Linear}(h_t)\n", "\\end{align}\n", "\n", "You will need to implement both the `forward` function and the `predict` function. You'll use the `nn.RNN` function to implement each time step of the RNN, with an explicit `for` loop to step through the time steps. (In the next part, you'll use a single call to `nn.RNN` to handle the entire process!)\n", "\n", "> Hint: you can reuse your `predict` implementation from before if you wrote it in a general way."]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "qZThl2l59nmt"}, "outputs": [], "source": ["class RNNTagger2(RNNBaseTagger):\n", "  def __init__(self, text, tag, embedding_size, hidden_size):\n", "    super().__init__()\n", "    self.text = text\n", "    self.tag = tag\n", "    self.N = len(tag.vocab.itos)   # tag vocab size\n", "    self.V = len(text.vocab.itos)  # text vocab size\n", "    self.embedding_size = embedding_size\n", "    self.hidden_size = hidden_size\n", "\n", "    # Create essential modules\n", "    self.word_embeddings = nn.Embedding(self.V, embedding_size) # Lookup layer\n", "    self.rnn = nn.RNN(input_size=embedding_size, hidden_size=hidden_size)\n", "    self.hidden2output = nn.Linear(hidden_size, self.N)\n", "\n", "    # Create loss function\n", "    pad_id = self.tag.vocab.stoi[self.tag.pad_token]\n", "    self.loss_function = nn.CrossEntropyLoss(reduction='sum', ignore_index=pad_id)\n", "\n", "    # Initialize parameters\n", "    self.init_parameters()\n", "\n", "  def forward(self, text_batch):\n", "    \"\"\"Performs forward, returns logits.\n", "    \n", "    Arguments: \n", "      text_batch: a tensor of size (seq_len, 1) \n", "    Returns:\n", "      logits: a tensor of size (seq_len, 1, self.N)\n", "    \"\"\"\n", "    # h0 shall be (num_layers * num_directions, batch, hidden_size),\n", "    # which is (1, 1, hidden_size)\n", "    h0 = torch.zeros(1, 1, self.hidden_size, device=device)\n", "    #TODO: your code below\n", "    logits = ...\n", "    return logits\n", "\n", "  def predict(self, text_batch):\n", "    \"\"\"Returns the most likely sequence of tags for a sequence of words in `text_batch`.\n", "\n", "    Arguments: \n", "      text_batch: a tensor containing word ids of size (seq_len, 1) \n", "    Returns:\n", "      tag_batch: a tensor containing tag ids of size (seq_len, 1)\n", "    \"\"\"\n", "    #TODO: your code below\n", "    tag_batch = ...\n", "    return tag_batch"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 425}, "id": "KeYqU--A91G4", "outputId": "06554248-d3c3-456a-ce88-2cef92f6bf7d"}, "outputs": [], "source": ["# Instantiate and train classifier\n", "rnn_tagger2 = RNNTagger2(TEXT, TAG, embedding_size=32, hidden_size=32).to(device)\n", "rnn_tagger2.train_all(train_iter, val_iter, epochs=5, learning_rate=1e-3)\n", "rnn_tagger2.load_state_dict(rnn_tagger2.best_model)\n", "\n", "# Evaluate model performance\n", "train_accuracy2, train_p2, train_r2, train_f2 = rnn_tagger2.evaluate(train_iter)\n", "test_accuracy2, test_p2, test_r2, test_f2 = rnn_tagger2.evaluate(test_iter)\n", "print(f'\\nTraining accuracy: {train_accuracy2:.3f}, precision: {train_p2:.3f}, recall: {train_r2:.3f}, F-1: {train_f2:.3f}\\n'\n", "      f'Test accuracy: {test_accuracy2:.3f}, precision: {test_p2:.3f}, recall: {test_r2:.3f}, F-1: {test_f2:.3f}')"]}, {"cell_type": "code", "execution_count": null, "id": "ba2bc506", "metadata": {"deletable": false, "editable": false}, "outputs": [], "source": ["grader.check(\"rnn2\")"]}, {"cell_type": "markdown", "metadata": {"id": "E4NhvF199qr4"}, "source": ["#### RNN forward using bidirectional `nn.RNN`\n", "\n", "Instead of using a for loop, we can directly feed the entire sequence to `nn.RNN`:\n", "\n", "\\begin{align}\n", "h_0 &= 0\\\\\n", "H &= \\text{nn.RNN}(X,h_0) \\\\\n", "O &= \\text{nn.Linear}(H)\n", "\\end{align}\n", "\n", "where $X$ is the concatenation of $x_1, \\cdots, x_T$, $H$ is the concatenation of $h_1, \\cdots, h_T$, and $O$ is the concatenation of $o_1, \\cdots, o_T$. \n", "\n", "By using this formulation, our code becomes more efficient, since `nn.RNN` is highly optimized. Besides, we can use bi-directional RNNs by simply passing `bidirectional=True` to the RNN constructor.\n", "\n", "The difference between a bidirectional RNN and a unidirectional RNN is that bidirectional RNNs have an additional RNN cell running in the reverse direction:\n", "\n", "\\begin{align}\n", "&h_{T+1}' = 0\\\\\n", "&h_t' = \\sigma(\\vect{U}'x_{t}' + \\vect{V}'h_{t + 1}' + b_h') \\\\\n", "\\end{align}\n", "\n", "To get the output at step $t$, a bidirectional RNN simply concatenates $h_t$ and $h_t'$ and projects to produce outputs. The benefit of a bidirectional RNN is that the output at step $t$ takes into account not only words $x_1,\\cdots, x_t$, but also $x_t, \\cdots, x_T$.\n", "\n", "Implement `forward` and `predict` functions below, using a bidirectional RNN."]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "TPWa4yKE9xYj"}, "outputs": [], "source": ["class RNNTagger3(RNNBaseTagger):\n", "  def __init__(self, text, tag, embedding_size, hidden_size):\n", "    super().__init__()\n", "    self.text = text\n", "    self.tag = tag\n", "    self.N = len(tag.vocab.itos)   # tag vocab size\n", "    self.V = len(text.vocab.itos)  # text vocab size\n", "    self.embedding_size = embedding_size\n", "    self.hidden_size = hidden_size\n", "\n", "    # Create essential modules\n", "    self.word_embeddings = nn.Embedding(self.V, embedding_size) # Lookup layer\n", "    self.rnn = nn.RNN(input_size=embedding_size, \n", "                      hidden_size=hidden_size,\n", "                      bidirectional=True)\n", "    self.hidden2output = nn.Linear(hidden_size*2, self.N)\n", "\n", "    # Create loss function\n", "    pad_id = self.tag.vocab.stoi[self.tag.pad_token]\n", "    self.loss_function = nn.CrossEntropyLoss(reduction='sum', ignore_index=pad_id)\n", "\n", "    # Initialize parameters\n", "    self.init_parameters()\n", "\n", "  def forward(self, text_batch):\n", "    \"\"\"Returns the most likely sequence of tags for a sequence of words in `text_batch`.\n", "\n", "    Arguments: \n", "      text_batch: a tensor containing word ids of size (seq_len, 1) \n", "    Returns:\n", "      tag_batch: a tensor containing tag ids of size (seq_len, 1)\n", "    \"\"\"\n", "    hidden = None # equivalent to setting hidden to a zero vector\n", "    #TODO: your code below\n", "    logits = ...\n", "    return logits\n", "\n", "  def predict(self, text_batch):\n", "    \"\"\"Returns the most likely sequence of tags for a sequence of words in `text_batch`.\n", "\n", "    Arguments: \n", "      text_batch: a tensor containing word ids of size (seq_len, 1) \n", "    Returns:\n", "      tag_batch: a tensor containing tag ids of size (seq_len, 1)\n", "    \"\"\"\n", "    #TODO: your code below\n", "    tag_batch = ...\n", "    return tag_batch"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 425}, "id": "ow_Jf55k922N", "outputId": "6f28c2cf-9b93-4c67-984f-e9c6a89ea3a1"}, "outputs": [], "source": ["# Instantiate and train classifier\n", "rnn_tagger3 = RNNTagger3(TEXT, TAG, embedding_size=32, hidden_size=32).to(device)\n", "rnn_tagger3.train_all(train_iter, val_iter, epochs=5, learning_rate=1e-3)\n", "rnn_tagger3.load_state_dict(rnn_tagger3.best_model)\n", "\n", "# Evaluate model performance\n", "train_accuracy3, train_p3, train_r3, train_f3 = rnn_tagger3.evaluate(train_iter)\n", "test_accuracy3, test_p3, test_r3, test_f3 = rnn_tagger3.evaluate(test_iter)\n", "print(f'\\nTraining accuracy: {train_accuracy3:.3f}, precision: {train_p3:.3f}, recall: {train_r3:.3f}, F-1: {train_f3:.3f}\\n'\n", "      f'Test accuracy: {test_accuracy3:.3f}, precision: {test_p3:.3f}, recall: {test_r3:.3f}, F-1: {test_f3:.3f}')"]}, {"cell_type": "code", "execution_count": null, "id": "f17ea72e", "metadata": {"deletable": false, "editable": false}, "outputs": [], "source": ["grader.check(\"birnn\")"]}, {"cell_type": "markdown", "metadata": {"id": "ANKdn-Mzg-qu"}, "source": ["Let's see what our model predicts for the example we used before."]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 108}, "id": "-vVanqR4hN4l", "outputId": "297b2c1c-758f-49f7-c30a-6feaf63126c7"}, "outputs": [], "source": ["# Pick one example\n", "example_id = 2 # the third example\n", "example = test_iter.dataset[example_id]\n", "\n", "# Process strings to word ids\n", "text_tensor = TEXT.process([example.text,]).to(device)\n", "\n", "# Predict\n", "prediction_tensor = rnn_tagger3.predict(text_tensor)\n", "prediction = [TAG.vocab.itos[tag_id] for tag_id in prediction_tensor[:, 0]]\n", "\n", "print ('Ground truth punctuation:')\n", "print(restore_punctuation(example.text, example.tag))\n", "print ('Predicted punctuation:')\n", "print(restore_punctuation(example.text, prediction))"]}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false, "id": "BG3qRSIkNZfR"}, "source": ["<!-- BEGIN QUESTION -->\n", "\n", "**Question:** Did your bidirectional RNN reach a higher F-1 score than unidirectional RNNs? Why?\n", "\n", "<!--\n", "BEGIN QUESTION\n", "name: open_response_birnn\n", "manual: true\n", "-->"]}, {"cell_type": "markdown", "id": "accea613", "metadata": {}, "source": ["_Type your answer here, replacing this text._"]}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false, "id": "5oMwMCysYtmc"}, "source": ["<!-- END QUESTION -->\n", "\n", "<!-- BEGIN QUESTION -->\n", "\n", "## Lab debrief \u2013 for consensus submission only\n", "\n", "**Question:** We're interested in any thoughts your group has about this lab so that we can improve this lab for later years, and to inform later labs for this year. Please list any issues that arose or comments you have to improve the lab. Useful things to comment on include the following: \n", "\n", "* Was the lab too long or too short?\n", "* Were the readings appropriate for the lab? \n", "* Was it clear (at least after you completed the lab) what the points of the exercises were? \n", "* Are there additions or changes you think would make the lab better?\n", "\n", "<!--\n", "BEGIN QUESTION\n", "name: open_response_debrief\n", "manual: true\n", "-->"]}, {"cell_type": "markdown", "id": "d6b10e38", "metadata": {}, "source": ["_Type your answer here, replacing this text._"]}, {"cell_type": "markdown", "metadata": {"id": "QouHfYzBYx-7"}, "source": ["<!-- END QUESTION -->\n", "\n", "\n", "\n", "# End of lab 2-5"]}, {"cell_type": "markdown", "id": "3dff33d7", "metadata": {"deletable": false, "editable": false}, "source": ["---\n", "\n", "To double-check your work, the cell below will rerun all of the autograder tests."]}, {"cell_type": "code", "execution_count": null, "id": "028c6771", "metadata": {"deletable": false, "editable": false}, "outputs": [], "source": ["grader.check_all()"]}], "metadata": {"colab": {"collapsed_sections": [], "name": "lab2-5.ipynb", "provenance": [], "toc_visible": true}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.10"}, "title": "CS187 Lab 2-5: Sequence labeling with recurrent neural networks"}, "nbformat": 4, "nbformat_minor": 4}